{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f12043-59f9-4de3-a74c-3c0c0c1ba3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting ULTRA-FAST training for 128-channel data on RTX 4070...\n",
      "Creating optimized datasets for 128-channel data...\n",
      "Preloading file indices for 128-channel data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning HDF5 files:  10%|â–ˆ         | 390/3799 [00:00<00:02, 1699.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 50000 samples with 128 channels\n",
      "Training on 40000 samples\n",
      "Validating on 10000 samples\n",
      "Image size: 128x128\n",
      "Number of channels: 128\n",
      "Batch size: 16\n",
      "Batches per epoch: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
      "- segformer.encoder.patch_embeddings.0.proj.weight: found shape torch.Size([32, 3, 7, 7]) in the checkpoint and torch.Size([32, 128, 7, 7]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SegFormer initialized for 128 input channels with upsampling\n",
      "Model parameters: 3,910,401\n",
      "Estimated GPU memory per batch: 0.12 GB\n",
      "\n",
      "ðŸŽ¯ 128-Channel Training Summary:\n",
      "   â€¢ Input shape: [B, 128, 128, 128]\n",
      "   â€¢ Output shape: [B, 1, 128, 128]\n",
      "   â€¢ Batch size: 16\n",
      "   â€¢ Estimated GPU memory: 0.12 GB\n",
      "   â€¢ Model adapted for 128 input channels with upsampling\n",
      "\n",
      "Starting ULTRA-FAST training for 128-channel data...\n",
      "Starting optimized training for 128-channel data with mixed precision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: torch.Size([16, 128, 128, 128])\n",
      "Input mask shape: torch.Size([16, 1, 128, 128])\n",
      "Image range: [-2.994, 7.077]\n",
      "Mask unique values: tensor([0., 1.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:57<00:00, 14.11it/s, loss=0.3490, avg_loss=0.3735, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50 (3.7 min):\n",
      "  Train Loss: 0.3731, Train IoU: 0.0000\n",
      "  Val Loss: 0.3529, Val IoU: 0.0000\n",
      "  LR: 9.99e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [03:03<00:00, 13.64it/s, loss=0.3456, avg_loss=0.3511, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50 (3.7 min):\n",
      "  Train Loss: 0.3511, Train IoU: 0.0086\n",
      "  Val Loss: 0.3470, Val IoU: 0.0226\n",
      "  LR: 9.96e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.0226)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [03:01<00:00, 13.81it/s, loss=0.3212, avg_loss=0.3455, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50 (3.6 min):\n",
      "  Train Loss: 0.3455, Train IoU: 0.0380\n",
      "  Val Loss: 0.3425, Val IoU: 0.0456\n",
      "  LR: 9.91e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.0456)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [03:03<00:00, 13.59it/s, loss=0.3205, avg_loss=0.3401, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50 (3.7 min):\n",
      "  Train Loss: 0.3401, Train IoU: 0.0553\n",
      "  Val Loss: 0.3397, Val IoU: 0.0630\n",
      "  LR: 9.84e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.0630)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:59<00:00, 13.96it/s, loss=0.3480, avg_loss=0.3351, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50 (3.6 min):\n",
      "  Train Loss: 0.3351, Train IoU: 0.0686\n",
      "  Val Loss: 0.3325, Val IoU: 0.0794\n",
      "  LR: 9.76e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.0794)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [03:01<00:00, 13.81it/s, loss=0.3696, avg_loss=0.3302, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50 (3.7 min):\n",
      "  Train Loss: 0.3302, Train IoU: 0.0834\n",
      "  Val Loss: 0.3291, Val IoU: 0.0744\n",
      "  LR: 9.65e-05\n",
      "  GPU Memory: 0.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [03:00<00:00, 13.83it/s, loss=0.3063, avg_loss=0.3246, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50 (3.7 min):\n",
      "  Train Loss: 0.3246, Train IoU: 0.0970\n",
      "  Val Loss: 0.3218, Val IoU: 0.1063\n",
      "  LR: 9.52e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.1063)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:59<00:00, 13.90it/s, loss=0.3201, avg_loss=0.3201, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50 (3.6 min):\n",
      "  Train Loss: 0.3199, Train IoU: 0.1096\n",
      "  Val Loss: 0.3194, Val IoU: 0.1086\n",
      "  LR: 9.38e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.1086)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 13.98it/s, loss=0.3464, avg_loss=0.3154, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50 (3.6 min):\n",
      "  Train Loss: 0.3153, Train IoU: 0.1201\n",
      "  Val Loss: 0.3131, Val IoU: 0.1135\n",
      "  LR: 9.22e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.1135)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [03:07<00:00, 13.31it/s, loss=0.3187, avg_loss=0.3105, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50 (3.8 min):\n",
      "  Train Loss: 0.3105, Train IoU: 0.1314\n",
      "  Val Loss: 0.3083, Val IoU: 0.1347\n",
      "  LR: 9.05e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.1347)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:56<00:00, 14.15it/s, loss=0.3186, avg_loss=0.3059, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50 (3.6 min):\n",
      "  Train Loss: 0.3060, Train IoU: 0.1420\n",
      "  Val Loss: 0.3022, Val IoU: 0.1474\n",
      "  LR: 8.85e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.1474)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:59<00:00, 13.92it/s, loss=0.2967, avg_loss=0.3029, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50 (3.6 min):\n",
      "  Train Loss: 0.3027, Train IoU: 0.1492\n",
      "  Val Loss: 0.3018, Val IoU: 0.1458\n",
      "  LR: 8.64e-05\n",
      "  GPU Memory: 0.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:57<00:00, 14.05it/s, loss=0.3345, avg_loss=0.2978, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50 (3.6 min):\n",
      "  Train Loss: 0.2977, Train IoU: 0.1604\n",
      "  Val Loss: 0.2973, Val IoU: 0.1559\n",
      "  LR: 8.42e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.1559)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 13.99it/s, loss=0.2871, avg_loss=0.2956, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50 (3.6 min):\n",
      "  Train Loss: 0.2954, Train IoU: 0.1654\n",
      "  Val Loss: 0.2920, Val IoU: 0.1713\n",
      "  LR: 8.19e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.1713)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:55<00:00, 14.23it/s, loss=0.2502, avg_loss=0.2911, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50 (3.5 min):\n",
      "  Train Loss: 0.2908, Train IoU: 0.1755\n",
      "  Val Loss: 0.2908, Val IoU: 0.1698\n",
      "  LR: 7.94e-05\n",
      "  GPU Memory: 0.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:59<00:00, 13.97it/s, loss=0.2440, avg_loss=0.2874, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50 (3.6 min):\n",
      "  Train Loss: 0.2874, Train IoU: 0.1827\n",
      "  Val Loss: 0.2839, Val IoU: 0.1869\n",
      "  LR: 7.68e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.1869)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:57<00:00, 14.05it/s, loss=0.2768, avg_loss=0.2834, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50 (3.6 min):\n",
      "  Train Loss: 0.2834, Train IoU: 0.1915\n",
      "  Val Loss: 0.2819, Val IoU: 0.1908\n",
      "  LR: 7.41e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.1908)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 13.99it/s, loss=0.3396, avg_loss=0.2804, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50 (3.6 min):\n",
      "  Train Loss: 0.2803, Train IoU: 0.1981\n",
      "  Val Loss: 0.2772, Val IoU: 0.2021\n",
      "  LR: 7.13e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2021)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 14.00it/s, loss=0.2434, avg_loss=0.2786, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50 (3.6 min):\n",
      "  Train Loss: 0.2785, Train IoU: 0.2015\n",
      "  Val Loss: 0.2769, Val IoU: 0.1994\n",
      "  LR: 6.84e-05\n",
      "  GPU Memory: 0.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:55<00:00, 14.21it/s, loss=0.2817, avg_loss=0.2742, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50 (3.6 min):\n",
      "  Train Loss: 0.2743, Train IoU: 0.2105\n",
      "  Val Loss: 0.2692, Val IoU: 0.2174\n",
      "  LR: 6.55e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2174)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:55<00:00, 14.21it/s, loss=0.2262, avg_loss=0.2706, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50 (3.5 min):\n",
      "  Train Loss: 0.2708, Train IoU: 0.2177\n",
      "  Val Loss: 0.2694, Val IoU: 0.2155\n",
      "  LR: 6.24e-05\n",
      "  GPU Memory: 0.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:56<00:00, 14.14it/s, loss=0.2588, avg_loss=0.2676, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50 (3.6 min):\n",
      "  Train Loss: 0.2677, Train IoU: 0.2241\n",
      "  Val Loss: 0.2660, Val IoU: 0.2256\n",
      "  LR: 5.94e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 14.03it/s, loss=0.2641, avg_loss=0.2649, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/50 (3.6 min):\n",
      "  Train Loss: 0.2647, Train IoU: 0.2301\n",
      "  Val Loss: 0.2622, Val IoU: 0.2310\n",
      "  LR: 5.63e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2310)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:57<00:00, 14.08it/s, loss=0.2358, avg_loss=0.2626, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/50 (3.6 min):\n",
      "  Train Loss: 0.2624, Train IoU: 0.2355\n",
      "  Val Loss: 0.2587, Val IoU: 0.2379\n",
      "  LR: 5.31e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2379)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:57<00:00, 14.12it/s, loss=0.3169, avg_loss=0.2597, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/50 (3.6 min):\n",
      "  Train Loss: 0.2596, Train IoU: 0.2409\n",
      "  Val Loss: 0.2566, Val IoU: 0.2437\n",
      "  LR: 5.00e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2437)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:59<00:00, 13.94it/s, loss=0.2435, avg_loss=0.2565, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/50 (3.6 min):\n",
      "  Train Loss: 0.2565, Train IoU: 0.2484\n",
      "  Val Loss: 0.2538, Val IoU: 0.2504\n",
      "  LR: 4.69e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2504)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 13.97it/s, loss=0.1902, avg_loss=0.2539, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/50 (3.6 min):\n",
      "  Train Loss: 0.2539, Train IoU: 0.2531\n",
      "  Val Loss: 0.2512, Val IoU: 0.2546\n",
      "  LR: 4.37e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2546)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:57<00:00, 14.06it/s, loss=0.2337, avg_loss=0.2519, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/50 (3.6 min):\n",
      "  Train Loss: 0.2518, Train IoU: 0.2575\n",
      "  Val Loss: 0.2473, Val IoU: 0.2677\n",
      "  LR: 4.06e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2677)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [03:01<00:00, 13.77it/s, loss=0.2579, avg_loss=0.2493, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/50 (3.7 min):\n",
      "  Train Loss: 0.2491, Train IoU: 0.2633\n",
      "  Val Loss: 0.2454, Val IoU: 0.2689\n",
      "  LR: 3.76e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2689)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:57<00:00, 14.12it/s, loss=0.2469, avg_loss=0.2466, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/50 (3.6 min):\n",
      "  Train Loss: 0.2466, Train IoU: 0.2690\n",
      "  Val Loss: 0.2428, Val IoU: 0.2747\n",
      "  LR: 3.45e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2747)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:57<00:00, 14.06it/s, loss=0.2095, avg_loss=0.2432, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/50 (3.6 min):\n",
      "  Train Loss: 0.2434, Train IoU: 0.2761\n",
      "  Val Loss: 0.2410, Val IoU: 0.2792\n",
      "  LR: 3.16e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2792)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 13.98it/s, loss=0.2748, avg_loss=0.2417, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/50 (3.6 min):\n",
      "  Train Loss: 0.2417, Train IoU: 0.2795\n",
      "  Val Loss: 0.2390, Val IoU: 0.2815\n",
      "  LR: 2.87e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2815)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 14.01it/s, loss=0.2350, avg_loss=0.2403, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/50 (3.6 min):\n",
      "  Train Loss: 0.2402, Train IoU: 0.2828\n",
      "  Val Loss: 0.2364, Val IoU: 0.2867\n",
      "  LR: 2.59e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2867)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 14.03it/s, loss=0.2544, avg_loss=0.2387, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34/50 (3.6 min):\n",
      "  Train Loss: 0.2385, Train IoU: 0.2860\n",
      "  Val Loss: 0.2334, Val IoU: 0.2939\n",
      "  LR: 2.32e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2939)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:56<00:00, 14.16it/s, loss=0.2518, avg_loss=0.2363, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35/50 (3.6 min):\n",
      "  Train Loss: 0.2363, Train IoU: 0.2906\n",
      "  Val Loss: 0.2328, Val IoU: 0.2948\n",
      "  LR: 2.06e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.2948)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 14.01it/s, loss=0.1515, avg_loss=0.2356, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/50 (3.6 min):\n",
      "  Train Loss: 0.2356, Train IoU: 0.2922\n",
      "  Val Loss: 0.2305, Val IoU: 0.3015\n",
      "  LR: 1.81e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.3015)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 13.99it/s, loss=0.2603, avg_loss=0.2329, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37/50 (3.6 min):\n",
      "  Train Loss: 0.2327, Train IoU: 0.2985\n",
      "  Val Loss: 0.2291, Val IoU: 0.3038\n",
      "  LR: 1.58e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.3038)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:59<00:00, 13.94it/s, loss=0.1876, avg_loss=0.2310, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38/50 (3.6 min):\n",
      "  Train Loss: 0.2312, Train IoU: 0.3025\n",
      "  Val Loss: 0.2282, Val IoU: 0.3087\n",
      "  LR: 1.36e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.3087)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:57<00:00, 14.05it/s, loss=0.2380, avg_loss=0.2304, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/50 (3.6 min):\n",
      "  Train Loss: 0.2304, Train IoU: 0.3042\n",
      "  Val Loss: 0.2263, Val IoU: 0.3114\n",
      "  LR: 1.15e-05\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.3114)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 14.03it/s, loss=0.2478, avg_loss=0.2295, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40/50 (3.7 min):\n",
      "  Train Loss: 0.2294, Train IoU: 0.3056\n",
      "  Val Loss: 0.2260, Val IoU: 0.3110\n",
      "  LR: 9.55e-06\n",
      "  GPU Memory: 0.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 14.01it/s, loss=0.2317, avg_loss=0.2280, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41/50 (3.6 min):\n",
      "  Train Loss: 0.2280, Train IoU: 0.3086\n",
      "  Val Loss: 0.2248, Val IoU: 0.3161\n",
      "  LR: 7.78e-06\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.3161)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:57<00:00, 14.10it/s, loss=0.2705, avg_loss=0.2278, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42/50 (3.6 min):\n",
      "  Train Loss: 0.2278, Train IoU: 0.3098\n",
      "  Val Loss: 0.2229, Val IoU: 0.3184\n",
      "  LR: 6.18e-06\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.3184)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 13.97it/s, loss=0.2269, avg_loss=0.2267, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43/50 (3.6 min):\n",
      "  Train Loss: 0.2266, Train IoU: 0.3120\n",
      "  Val Loss: 0.2231, Val IoU: 0.3178\n",
      "  LR: 4.76e-06\n",
      "  GPU Memory: 0.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:59<00:00, 13.92it/s, loss=0.2275, avg_loss=0.2262, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44/50 (3.6 min):\n",
      "  Train Loss: 0.2260, Train IoU: 0.3136\n",
      "  Val Loss: 0.2226, Val IoU: 0.3174\n",
      "  LR: 3.51e-06\n",
      "  GPU Memory: 0.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 13.97it/s, loss=0.2551, avg_loss=0.2263, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45/50 (3.6 min):\n",
      "  Train Loss: 0.2263, Train IoU: 0.3126\n",
      "  Val Loss: 0.2221, Val IoU: 0.3200\n",
      "  LR: 2.45e-06\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.3200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:59<00:00, 13.96it/s, loss=0.2697, avg_loss=0.2249, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46/50 (3.6 min):\n",
      "  Train Loss: 0.2248, Train IoU: 0.3159\n",
      "  Val Loss: 0.2214, Val IoU: 0.3217\n",
      "  LR: 1.57e-06\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.3217)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 14.02it/s, loss=0.1942, avg_loss=0.2251, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47/50 (3.6 min):\n",
      "  Train Loss: 0.2252, Train IoU: 0.3154\n",
      "  Val Loss: 0.2208, Val IoU: 0.3238\n",
      "  LR: 8.86e-07\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.3238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:59<00:00, 13.93it/s, loss=0.2716, avg_loss=0.2252, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48/50 (3.6 min):\n",
      "  Train Loss: 0.2252, Train IoU: 0.3149\n",
      "  Val Loss: 0.2208, Val IoU: 0.3232\n",
      "  LR: 3.94e-07\n",
      "  GPU Memory: 0.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:58<00:00, 14.02it/s, loss=0.1998, avg_loss=0.2243, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49/50 (3.6 min):\n",
      "  Train Loss: 0.2242, Train IoU: 0.3176\n",
      "  Val Loss: 0.2209, Val IoU: 0.3220\n",
      "  LR: 9.87e-08\n",
      "  GPU Memory: 0.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [02:56<00:00, 14.15it/s, loss=0.1898, avg_loss=0.2246, GPU_mem=0.20GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50/50 (3.6 min):\n",
      "  Train Loss: 0.2247, Train IoU: 0.3162\n",
      "  Val Loss: 0.2205, Val IoU: 0.3241\n",
      "  LR: 0.00e+00\n",
      "  GPU Memory: 0.20GB\n",
      "  âœ“ Saved best model (val_iou: 0.3241)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import SegformerConfig, SegformerForSemanticSegmentation\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# ======================\n",
    "# 1. FIXED SegFormer Model with Upsampling\n",
    "# ======================\n",
    "\n",
    "class SegFormerLandslide128(nn.Module):\n",
    "    def __init__(self, num_classes=1, input_size=128, output_size=128):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Load configuration and modify for 128 channels and num_classes\n",
    "        config = SegformerConfig.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "        config.num_channels = 128  # CRITICAL: Change from 3 to 128\n",
    "        config.num_labels = num_classes  # Set number of output classes\n",
    "        \n",
    "        # Load model with modified configuration\n",
    "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
    "            config=config,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Add upsampling layer to match output size\n",
    "        self.upsample = nn.Upsample(size=output_size, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        print(\"âœ“ SegFormer initialized for 128 input channels with upsampling\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 128, height, width]\n",
    "        outputs = self.segformer(x).logits  # [B, 1, 32, 32] for input [B, 128, 128, 128]\n",
    "        return self.upsample(outputs)  # [B, 1, 128, 128]\n",
    "\n",
    "# ======================\n",
    "# 2. Updated Loss Function with Size Handling\n",
    "# ======================\n",
    "\n",
    "class LandslideLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def dice_loss(self, pred, target):\n",
    "        smooth = 1.0\n",
    "        pred = torch.sigmoid(pred)\n",
    "        \n",
    "        # Ensure same spatial dimensions\n",
    "        if pred.shape[-2:] != target.shape[-2:]:\n",
    "            pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=True)\n",
    "            \n",
    "        intersection = (pred * target).sum()\n",
    "        union = pred.sum() + target.sum()\n",
    "        return 1.0 - (2.0 * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Ensure pred and target have same spatial dimensions\n",
    "        if pred.shape[-2:] != target.shape[-2:]:\n",
    "            pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=True)\n",
    "            \n",
    "        bce_loss = self.bce(pred, target)\n",
    "        dice_loss = self.dice_loss(pred, target)\n",
    "        return self.alpha * bce_loss + (1 - self.alpha) * dice_loss\n",
    "\n",
    "def calculate_iou(pred, target):\n",
    "    pred = (torch.sigmoid(pred) > 0.5).float()\n",
    "    \n",
    "    # Ensure same spatial dimensions\n",
    "    if pred.shape[-2:] != target.shape[-2:]:\n",
    "        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        \n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    return (intersection + 1e-6) / (union + 1e-6)\n",
    "\n",
    "# ======================\n",
    "# 3. Dataset Class (Same as before)\n",
    "# ======================\n",
    "\n",
    "class OptimizedLandslideH5Dataset128(Dataset):\n",
    "    def __init__(self, img_h5_paths, mask_h5_paths=None, img_size=128, is_train=True, max_samples=None):\n",
    "        self.img_h5_paths = img_h5_paths\n",
    "        self.mask_h5_paths = mask_h5_paths\n",
    "        self.img_size = img_size\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # We'll handle transforms manually for 128-channel data\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # PRELOAD all file indices for maximum speed\n",
    "        self.total_samples = 0\n",
    "        self.file_indices = []\n",
    "        \n",
    "        print(\"Preloading file indices for 128-channel data...\")\n",
    "        for file_idx, img_path in enumerate(tqdm(img_h5_paths, desc=\"Scanning HDF5 files\")):\n",
    "            try:\n",
    "                with h5py.File(img_path, 'r') as img_file:\n",
    "                    img_key = list(img_file.keys())[0]\n",
    "                    n_samples = img_file[img_key].shape[0]\n",
    "                    \n",
    "                    for sample_idx in range(n_samples):\n",
    "                        if max_samples and self.total_samples >= max_samples:\n",
    "                            break\n",
    "                        self.file_indices.append((file_idx, sample_idx))\n",
    "                        self.total_samples += 1\n",
    "                    \n",
    "                    if max_samples and self.total_samples >= max_samples:\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"âœ“ Loaded {self.total_samples} samples with 128 channels\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "    \n",
    "    def _apply_transforms(self, image, mask):\n",
    "        \"\"\"Manual transform for 128-channel data\"\"\"\n",
    "        # Resize\n",
    "        if image.shape[0] != self.img_size or image.shape[1] != self.img_size:\n",
    "            # Resize each channel separately\n",
    "            resized_image = np.zeros((self.img_size, self.img_size, image.shape[2]), dtype=np.float32)\n",
    "            for c in range(image.shape[2]):\n",
    "                resized_image[:, :, c] = cv2.resize(image[:, :, c], (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)\n",
    "            image = resized_image\n",
    "        \n",
    "        if mask.shape[0] != self.img_size or mask.shape[1] != self.img_size:\n",
    "            mask = cv2.resize(mask, (self.img_size, self.img_size), interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # Data augmentation for training\n",
    "        if self.is_train:\n",
    "            # Horizontal flip\n",
    "            if np.random.random() > 0.5:\n",
    "                image = np.fliplr(image).copy()\n",
    "                mask = np.fliplr(mask).copy()\n",
    "            \n",
    "            # Vertical flip\n",
    "            if np.random.random() > 0.5:\n",
    "                image = np.flipud(image).copy()\n",
    "                mask = np.flipud(mask).copy()\n",
    "        \n",
    "        # Convert to PyTorch format: (channels, height, width)\n",
    "        image = torch.from_numpy(image).float().permute(2, 0, 1)\n",
    "        mask = torch.from_numpy(mask).float().unsqueeze(0)\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, sample_idx = self.file_indices[idx]\n",
    "        img_path = self.img_h5_paths[file_idx]\n",
    "        mask_path = self.mask_h5_paths[file_idx] if self.mask_h5_paths else None\n",
    "        \n",
    "        try:\n",
    "            # Load 128-channel image\n",
    "            with h5py.File(img_path, 'r') as img_file:\n",
    "                img_key = list(img_file.keys())[0]\n",
    "                img = img_file[img_key][sample_idx]  # Shape could be (128, 128, 128) or (128, 128)\n",
    "                \n",
    "                # Handle different possible shapes\n",
    "                if len(img.shape) == 2:\n",
    "                    # Single channel - repeat to 128 channels (shouldn't happen but safe)\n",
    "                    img = np.stack([img] * 128, axis=-1)\n",
    "                elif len(img.shape) == 3:\n",
    "                    # If shape is (128, 128, 128) - perfect\n",
    "                    if img.shape[0] == 128 and img.shape[2] == 128:\n",
    "                        # Shape is (128, height, width) - channels first, convert to channels last\n",
    "                        img = np.transpose(img, (1, 2, 0))\n",
    "                \n",
    "                # Ensure we have exactly 128 channels\n",
    "                if img.shape[-1] > 128:\n",
    "                    img = img[:, :, :128]\n",
    "                elif img.shape[-1] < 128:\n",
    "                    # Pad with zeros if fewer channels\n",
    "                    padding = np.zeros((img.shape[0], img.shape[1], 128 - img.shape[-1]), dtype=img.dtype)\n",
    "                    img = np.concatenate([img, padding], axis=-1)\n",
    "                \n",
    "                # Convert to float32 and normalize\n",
    "                img = img.astype(np.float32)\n",
    "                \n",
    "                # Normalize each channel separately\n",
    "                for c in range(img.shape[-1]):\n",
    "                    channel_data = img[:, :, c]\n",
    "                    if np.std(channel_data) > 0:\n",
    "                        img[:, :, c] = (channel_data - np.mean(channel_data)) / np.std(channel_data)\n",
    "                \n",
    "                # Load mask\n",
    "                mask = np.zeros((img.shape[0], img.shape[1]), dtype=np.float32)\n",
    "                if mask_path and os.path.exists(mask_path):\n",
    "                    with h5py.File(mask_path, 'r') as mask_file:\n",
    "                        mask_key = list(mask_file.keys())[0]\n",
    "                        mask_data = mask_file[mask_key]\n",
    "                        \n",
    "                        if len(mask_data.shape) == 3 and sample_idx < mask_data.shape[0]:\n",
    "                            mask = mask_data[sample_idx]\n",
    "                        elif len(mask_data.shape) == 2:\n",
    "                            mask = mask_data[:]\n",
    "                        \n",
    "                        if len(mask.shape) == 3:\n",
    "                            mask = mask.squeeze()\n",
    "                        mask = (mask > 0).astype(np.float32)\n",
    "                \n",
    "                # Apply transforms\n",
    "                image_tensor, mask_tensor = self._apply_transforms(img, mask)\n",
    "                \n",
    "                return {\n",
    "                    'image': image_tensor,  # Shape: [128, 128, 128]\n",
    "                    'mask': mask_tensor    # Shape: [1, 128, 128]\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx}: {e}\")\n",
    "            # Fast fallback - zero tensor with 128 channels\n",
    "            dummy_img = torch.zeros(128, self.img_size, self.img_size)\n",
    "            dummy_mask = torch.zeros(1, self.img_size, self.img_size)\n",
    "            return {'image': dummy_img, 'mask': dummy_mask}\n",
    "\n",
    "# ======================\n",
    "# 4. Updated Training Function with Modern AMP\n",
    "# ======================\n",
    "\n",
    "def optimized_train_model_128(model, train_loader, val_loader, epochs=50, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Modern mixed precision for RTX 4070\n",
    "    scaler = torch.amp.GradScaler(device)  # Updated for modern PyTorch\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    criterion = LandslideLoss(alpha=0.7)\n",
    "    \n",
    "    best_val_iou = 0.0\n",
    "    \n",
    "    print(\"Starting optimized training for 128-channel data with mixed precision...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training with mixed precision\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        \n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_bar):\n",
    "            images = batch['image'].to(device, non_blocking=True)\n",
    "            masks = batch['mask'].to(device, non_blocking=True)\n",
    "            \n",
    "            # Debug: Print shapes on first batch\n",
    "            if batch_idx == 0 and epoch == 0:\n",
    "                print(f\"Input image shape: {images.shape}\")  # Should be [B, 128, H, W]\n",
    "                print(f\"Input mask shape: {masks.shape}\")    # Should be [B, 1, H, W]\n",
    "                print(f\"Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "                print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Modern mixed precision forward\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Mixed precision backward\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_iou += calculate_iou(outputs, masks).item()\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                train_bar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'avg_loss': f'{train_loss/(batch_idx+1):.4f}',\n",
    "                    'GPU_mem': f'{torch.cuda.memory_allocated()/1024**3:.2f}GB'\n",
    "                })\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_iou = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(device, non_blocking=True)\n",
    "                masks = batch['mask'].to(device, non_blocking=True)\n",
    "                \n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, masks)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_iou += calculate_iou(outputs, masks).item()\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_iou = train_iou / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_iou = val_iou / len(val_loader)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}/{epochs} ({epoch_time/60:.1f} min):')\n",
    "        print(f'  Train Loss: {avg_train_loss:.4f}, Train IoU: {avg_train_iou:.4f}')\n",
    "        print(f'  Val Loss: {avg_val_loss:.4f}, Val IoU: {avg_val_iou:.4f}')\n",
    "        print(f'  LR: {scheduler.get_last_lr()[0]:.2e}')\n",
    "        print(f'  GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB')\n",
    "        \n",
    "        if avg_val_iou > best_val_iou:\n",
    "            best_val_iou = avg_val_iou\n",
    "            torch.save(model.state_dict(), 'best_landslide_model_128ch.pth')\n",
    "            print(f'  âœ“ Saved best model (val_iou: {avg_val_iou:.4f})')\n",
    "\n",
    "# ======================\n",
    "# 5. ULTRA-FAST TRAINING for 128-Channel Data\n",
    "# ======================\n",
    "\n",
    "def ultra_fast_training_128():\n",
    "    \"\"\"Ultra-fast training for 128-channel data\"\"\"\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"ðŸš€ Starting ULTRA-FAST training for 128-channel data on RTX 4070...\")\n",
    "    \n",
    "    base_path = \"/home/neel/Geog_project/Landslide4Sense_1\"\n",
    "    \n",
    "    def find_h5_files(directory):\n",
    "        if not os.path.exists(directory):\n",
    "            return []\n",
    "        return sorted([os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.h5')])\n",
    "    \n",
    "    train_img_files = find_h5_files(os.path.join(base_path, \"TrainData\", \"img\"))\n",
    "    train_mask_files = find_h5_files(os.path.join(base_path, \"TrainData\", \"mask\"))\n",
    "    \n",
    "    # Adjusted for 128 channels - smaller batch size due to increased memory\n",
    "    IMG_SIZE = 128\n",
    "    BATCH_SIZE = 16  # Reduced from 24 due to 128 channels\n",
    "    SUBSET_SIZE = 50000  # Reduced due to memory constraints\n",
    "    NUM_WORKERS = min(os.cpu_count(), 16)\n",
    "    \n",
    "    print(\"Creating optimized datasets for 128-channel data...\")\n",
    "    train_dataset = OptimizedLandslideH5Dataset128(\n",
    "        img_h5_paths=train_img_files,\n",
    "        mask_h5_paths=train_mask_files,\n",
    "        img_size=IMG_SIZE,\n",
    "        max_samples=SUBSET_SIZE,\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    # Split for validation (80/20)\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Ultra-fast dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_subset, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_subset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Training on {len(train_subset)} samples\")\n",
    "    print(f\"Validating on {len(val_subset)} samples\")\n",
    "    print(f\"Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "    print(f\"Number of channels: 128\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "    \n",
    "    # Model for 128 channels\n",
    "    model = SegFormerLandslide128(num_classes=1, input_size=IMG_SIZE, output_size=IMG_SIZE)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model parameters: {total_params:,}\")\n",
    "    \n",
    "    # Memory estimation for 128 channels\n",
    "    memory_per_sample = 128 * IMG_SIZE * IMG_SIZE * 4  # bytes for float32\n",
    "    batch_memory = BATCH_SIZE * memory_per_sample / (1024**3)  # GB\n",
    "    print(f\"Estimated GPU memory per batch: {batch_memory:.2f} GB\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ 128-Channel Training Summary:\")\n",
    "    print(f\"   â€¢ Input shape: [B, 128, {IMG_SIZE}, {IMG_SIZE}]\")\n",
    "    print(f\"   â€¢ Output shape: [B, 1, {IMG_SIZE}, {IMG_SIZE}]\")\n",
    "    print(f\"   â€¢ Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"   â€¢ Estimated GPU memory: {batch_memory:.2f} GB\")\n",
    "    print(f\"   â€¢ Model adapted for 128 input channels with upsampling\")\n",
    "    \n",
    "    print(\"\\nStarting ULTRA-FAST training for 128-channel data...\")\n",
    "    optimized_train_model_128(model, train_loader, val_loader, epochs=50, device=device)\n",
    "\n",
    "# Run the modified training\n",
    "if __name__ == \"__main__\":\n",
    "    ultra_fast_training_128()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f96071a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
      "- segformer.encoder.patch_embeddings.0.proj.weight: found shape torch.Size([32, 3, 7, 7]) in the checkpoint and torch.Size([32, 128, 7, 7]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SegFormer initialized for 128 input channels with upsampling\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m model.eval() \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# --- 2. Get a sample from your validation set ---\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# (Assuming 'val_loader' is your validation DataLoader)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m sample_batch = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mval_loader\u001b[49m))\n\u001b[32m     15\u001b[39m image_tensor = sample_batch[\u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     16\u001b[39m true_mask_tensor = sample_batch[\u001b[33m'\u001b[39m\u001b[33mmask\u001b[39m\u001b[33m'\u001b[39m].to(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'val_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Load your best model ---\n",
    "device = torch.device('cuda')\n",
    "model = SegFormerLandslide128(num_classes=1) # Use your model class\n",
    "model.load_state_dict(torch.load('best_landslide_model_128ch.pth'))\n",
    "model.to(device)\n",
    "model.eval() # Set model to evaluation mode\n",
    "\n",
    "# --- 2. Get a sample from your validation set ---\n",
    "# (Assuming 'val_loader' is your validation DataLoader)\n",
    "sample_batch = next(iter(val_loader))\n",
    "image_tensor = sample_batch['image'].to(device)\n",
    "true_mask_tensor = sample_batch['mask'].to(device)\n",
    "\n",
    "# --- 3. Make a prediction ---\n",
    "with torch.no_grad():\n",
    "    predicted_logits = model(image_tensor)\n",
    "    predicted_mask = (torch.sigmoid(predicted_logits) > 0.5).float()\n",
    "\n",
    "# --- 4. Visualize the first image in the batch ---\n",
    "# Move tensors to CPU and convert to numpy\n",
    "# Note: For visualizing a 128-channel image, we often just show an RGB composite (e.g., channels 3, 2, 1)\n",
    "image_to_show = image_tensor[0].cpu().numpy()\n",
    "rgb_composite = np.stack([image_to_show[3], image_to_show[2], image_to_show[1]], axis=-1)\n",
    "# Normalize for display\n",
    "rgb_composite = (rgb_composite - rgb_composite.min()) / (rgb_composite.max() - rgb_composite.min())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(rgb_composite)\n",
    "plt.title(\"Input Image (RGB Composite)\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(true_mask_tensor[0].squeeze().cpu().numpy(), cmap='gray')\n",
    "plt.title(\"Ground Truth\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(predicted_mask[0].squeeze().cpu().numpy(), cmap='gray')\n",
    "plt.title(f\"Model Prediction (IoU: {calculate_iou(predicted_logits[0], true_mask_tensor[0]):.4f})\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c89bc26-d9c4-4b79-bd3f-e14d11337e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting ULTRA-FAST training on RTX 4070...\n",
      "Creating optimized datasets...\n",
      "Preloading file indices for fast access...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning HDF5 files:   8%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                    | 312/3799 [00:00<00:00, 6145.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 40000 samples\n",
      "Training on 32000 samples\n",
      "Validating on 8000 samples\n",
      "Image size: 128x128\n",
      "Batch size: 24\n",
      "Batches per epoch: 1334\n",
      "Expected epoch time: 61.4 seconds (1.0 minutes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SegFormer initialized with pre-trained weights + upsampling\n",
      "Model parameters: 3,714,401\n",
      "\n",
      "ðŸŽ¯ Training Summary:\n",
      "   â€¢ Expected epoch time: 61.4 seconds\n",
      "   â€¢ Full training (50 epochs): 51.1 minutes\n",
      "   â€¢ GPU Memory usage: ~0.6GB\n",
      "   â€¢ Quality: Good for rapid prototyping\n",
      "\n",
      "Starting ULTRA-FAST training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1544750/1890320624.py:307: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimized training with mixed precision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|                                                                              | 0/1334 [00:00<?, ?it/s]/tmp/ipykernel_1544750/1890320624.py:334: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:41<00:00, 32.05it/s, loss=0.3602, avg_loss=0.3973]\n",
      "/tmp/ipykernel_1544750/1890320624.py:364: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50 (0.8 min):\n",
      "  Train Loss: 0.3964, Train IoU: 0.0007\n",
      "  Val Loss: 0.3633, Val IoU: 0.0000\n",
      "  LR: 9.99e-05\n",
      "  âœ“ Saved best model (val_iou: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:44<00:00, 30.23it/s, loss=0.3494, avg_loss=0.3608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50 (0.8 min):\n",
      "  Train Loss: 0.3607, Train IoU: 0.0000\n",
      "  Val Loss: 0.3625, Val IoU: 0.0000\n",
      "  LR: 9.96e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:40<00:00, 32.88it/s, loss=0.3668, avg_loss=0.3541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50 (0.7 min):\n",
      "  Train Loss: 0.3540, Train IoU: 0.0057\n",
      "  Val Loss: 0.3524, Val IoU: 0.0254\n",
      "  LR: 9.91e-05\n",
      "  âœ“ Saved best model (val_iou: 0.0254)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:42<00:00, 31.64it/s, loss=0.3619, avg_loss=0.3454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50 (0.8 min):\n",
      "  Train Loss: 0.3453, Train IoU: 0.0386\n",
      "  Val Loss: 0.3456, Val IoU: 0.0706\n",
      "  LR: 9.84e-05\n",
      "  âœ“ Saved best model (val_iou: 0.0706)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:45<00:00, 29.04it/s, loss=0.3438, avg_loss=0.3379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50 (0.9 min):\n",
      "  Train Loss: 0.3378, Train IoU: 0.0621\n",
      "  Val Loss: 0.3332, Val IoU: 0.0900\n",
      "  LR: 9.76e-05\n",
      "  âœ“ Saved best model (val_iou: 0.0900)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:44<00:00, 30.05it/s, loss=0.3477, avg_loss=0.3300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50 (0.8 min):\n",
      "  Train Loss: 0.3302, Train IoU: 0.0830\n",
      "  Val Loss: 0.3253, Val IoU: 0.1003\n",
      "  LR: 9.65e-05\n",
      "  âœ“ Saved best model (val_iou: 0.1003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:44<00:00, 29.73it/s, loss=0.2949, avg_loss=0.3239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50 (0.8 min):\n",
      "  Train Loss: 0.3238, Train IoU: 0.0995\n",
      "  Val Loss: 0.3225, Val IoU: 0.1146\n",
      "  LR: 9.52e-05\n",
      "  âœ“ Saved best model (val_iou: 0.1146)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:46<00:00, 28.50it/s, loss=0.3007, avg_loss=0.3181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50 (0.9 min):\n",
      "  Train Loss: 0.3181, Train IoU: 0.1120\n",
      "  Val Loss: 0.3138, Val IoU: 0.1251\n",
      "  LR: 9.38e-05\n",
      "  âœ“ Saved best model (val_iou: 0.1251)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:55<00:00, 23.90it/s, loss=0.3620, avg_loss=0.3109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50 (1.0 min):\n",
      "  Train Loss: 0.3110, Train IoU: 0.1301\n",
      "  Val Loss: 0.3058, Val IoU: 0.1444\n",
      "  LR: 9.22e-05\n",
      "  âœ“ Saved best model (val_iou: 0.1444)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:48<00:00, 27.28it/s, loss=0.2802, avg_loss=0.3053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50 (0.9 min):\n",
      "  Train Loss: 0.3052, Train IoU: 0.1429\n",
      "  Val Loss: 0.3025, Val IoU: 0.1464\n",
      "  LR: 9.05e-05\n",
      "  âœ“ Saved best model (val_iou: 0.1464)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:45<00:00, 29.53it/s, loss=0.3034, avg_loss=0.3013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50 (0.8 min):\n",
      "  Train Loss: 0.3015, Train IoU: 0.1506\n",
      "  Val Loss: 0.3032, Val IoU: 0.1573\n",
      "  LR: 8.85e-05\n",
      "  âœ“ Saved best model (val_iou: 0.1573)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:47<00:00, 27.98it/s, loss=0.3276, avg_loss=0.2972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50 (0.9 min):\n",
      "  Train Loss: 0.2971, Train IoU: 0.1598\n",
      "  Val Loss: 0.2915, Val IoU: 0.1733\n",
      "  LR: 8.64e-05\n",
      "  âœ“ Saved best model (val_iou: 0.1733)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:44<00:00, 29.69it/s, loss=0.2811, avg_loss=0.2919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50 (0.8 min):\n",
      "  Train Loss: 0.2919, Train IoU: 0.1706\n",
      "  Val Loss: 0.2946, Val IoU: 0.1706\n",
      "  LR: 8.42e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:47<00:00, 28.33it/s, loss=0.3049, avg_loss=0.2869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50 (0.9 min):\n",
      "  Train Loss: 0.2870, Train IoU: 0.1818\n",
      "  Val Loss: 0.2831, Val IoU: 0.1950\n",
      "  LR: 8.19e-05\n",
      "  âœ“ Saved best model (val_iou: 0.1950)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:49<00:00, 26.83it/s, loss=0.2491, avg_loss=0.2839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50 (0.9 min):\n",
      "  Train Loss: 0.2839, Train IoU: 0.1883\n",
      "  Val Loss: 0.2790, Val IoU: 0.1973\n",
      "  LR: 7.94e-05\n",
      "  âœ“ Saved best model (val_iou: 0.1973)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:49<00:00, 26.72it/s, loss=0.2800, avg_loss=0.2791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50 (0.9 min):\n",
      "  Train Loss: 0.2791, Train IoU: 0.1983\n",
      "  Val Loss: 0.2738, Val IoU: 0.2145\n",
      "  LR: 7.68e-05\n",
      "  âœ“ Saved best model (val_iou: 0.2145)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:46<00:00, 28.94it/s, loss=0.3280, avg_loss=0.2748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50 (0.9 min):\n",
      "  Train Loss: 0.2748, Train IoU: 0.2076\n",
      "  Val Loss: 0.2725, Val IoU: 0.2137\n",
      "  LR: 7.41e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:43<00:00, 30.45it/s, loss=0.2867, avg_loss=0.2714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50 (0.8 min):\n",
      "  Train Loss: 0.2715, Train IoU: 0.2143\n",
      "  Val Loss: 0.2691, Val IoU: 0.2202\n",
      "  LR: 7.13e-05\n",
      "  âœ“ Saved best model (val_iou: 0.2202)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:49<00:00, 26.76it/s, loss=0.2846, avg_loss=0.2669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50 (0.9 min):\n",
      "  Train Loss: 0.2669, Train IoU: 0.2242\n",
      "  Val Loss: 0.2654, Val IoU: 0.2302\n",
      "  LR: 6.84e-05\n",
      "  âœ“ Saved best model (val_iou: 0.2302)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:48<00:00, 27.75it/s, loss=0.2823, avg_loss=0.2641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50 (0.9 min):\n",
      "  Train Loss: 0.2643, Train IoU: 0.2289\n",
      "  Val Loss: 0.2624, Val IoU: 0.2370\n",
      "  LR: 6.55e-05\n",
      "  âœ“ Saved best model (val_iou: 0.2370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:48<00:00, 27.36it/s, loss=0.2693, avg_loss=0.2613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50 (0.9 min):\n",
      "  Train Loss: 0.2612, Train IoU: 0.2357\n",
      "  Val Loss: 0.2559, Val IoU: 0.2520\n",
      "  LR: 6.24e-05\n",
      "  âœ“ Saved best model (val_iou: 0.2520)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:45<00:00, 29.16it/s, loss=0.2594, avg_loss=0.2578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50 (0.8 min):\n",
      "  Train Loss: 0.2579, Train IoU: 0.2428\n",
      "  Val Loss: 0.2525, Val IoU: 0.2555\n",
      "  LR: 5.94e-05\n",
      "  âœ“ Saved best model (val_iou: 0.2555)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:45<00:00, 29.54it/s, loss=0.2085, avg_loss=0.2554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/50 (0.8 min):\n",
      "  Train Loss: 0.2556, Train IoU: 0.2475\n",
      "  Val Loss: 0.2475, Val IoU: 0.2664\n",
      "  LR: 5.63e-05\n",
      "  âœ“ Saved best model (val_iou: 0.2664)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:47<00:00, 27.82it/s, loss=0.2635, avg_loss=0.2520]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/50 (0.9 min):\n",
      "  Train Loss: 0.2520, Train IoU: 0.2552\n",
      "  Val Loss: 0.2458, Val IoU: 0.2714\n",
      "  LR: 5.31e-05\n",
      "  âœ“ Saved best model (val_iou: 0.2714)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:50<00:00, 26.60it/s, loss=0.2994, avg_loss=0.2489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/50 (0.9 min):\n",
      "  Train Loss: 0.2488, Train IoU: 0.2618\n",
      "  Val Loss: 0.2433, Val IoU: 0.2735\n",
      "  LR: 5.00e-05\n",
      "  âœ“ Saved best model (val_iou: 0.2735)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:49<00:00, 27.01it/s, loss=0.1831, avg_loss=0.2463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/50 (0.9 min):\n",
      "  Train Loss: 0.2462, Train IoU: 0.2673\n",
      "  Val Loss: 0.2410, Val IoU: 0.2800\n",
      "  LR: 4.69e-05\n",
      "  âœ“ Saved best model (val_iou: 0.2800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:44<00:00, 30.19it/s, loss=0.2084, avg_loss=0.2428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/50 (0.8 min):\n",
      "  Train Loss: 0.2426, Train IoU: 0.2753\n",
      "  Val Loss: 0.2371, Val IoU: 0.2877\n",
      "  LR: 4.37e-05\n",
      "  âœ“ Saved best model (val_iou: 0.2877)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:44<00:00, 29.91it/s, loss=0.2357, avg_loss=0.2402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/50 (0.8 min):\n",
      "  Train Loss: 0.2402, Train IoU: 0.2805\n",
      "  Val Loss: 0.2355, Val IoU: 0.2945\n",
      "  LR: 4.06e-05\n",
      "  âœ“ Saved best model (val_iou: 0.2945)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:48<00:00, 27.33it/s, loss=0.2663, avg_loss=0.2387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/50 (0.9 min):\n",
      "  Train Loss: 0.2384, Train IoU: 0.2848\n",
      "  Val Loss: 0.2333, Val IoU: 0.2951\n",
      "  LR: 3.76e-05\n",
      "  âœ“ Saved best model (val_iou: 0.2951)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:50<00:00, 26.42it/s, loss=0.1963, avg_loss=0.2363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/50 (0.9 min):\n",
      "  Train Loss: 0.2363, Train IoU: 0.2888\n",
      "  Val Loss: 0.2284, Val IoU: 0.3077\n",
      "  LR: 3.45e-05\n",
      "  âœ“ Saved best model (val_iou: 0.3077)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:46<00:00, 28.82it/s, loss=0.2300, avg_loss=0.2323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/50 (0.9 min):\n",
      "  Train Loss: 0.2322, Train IoU: 0.2981\n",
      "  Val Loss: 0.2295, Val IoU: 0.3036\n",
      "  LR: 3.16e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:45<00:00, 29.45it/s, loss=0.2150, avg_loss=0.2324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/50 (0.8 min):\n",
      "  Train Loss: 0.2326, Train IoU: 0.2968\n",
      "  Val Loss: 0.2254, Val IoU: 0.3142\n",
      "  LR: 2.87e-05\n",
      "  âœ“ Saved best model (val_iou: 0.3142)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:47<00:00, 28.19it/s, loss=0.1719, avg_loss=0.2296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/50 (0.9 min):\n",
      "  Train Loss: 0.2294, Train IoU: 0.3042\n",
      "  Val Loss: 0.2237, Val IoU: 0.3181\n",
      "  LR: 2.59e-05\n",
      "  âœ“ Saved best model (val_iou: 0.3181)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:47<00:00, 27.95it/s, loss=0.2324, avg_loss=0.2289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34/50 (0.9 min):\n",
      "  Train Loss: 0.2289, Train IoU: 0.3046\n",
      "  Val Loss: 0.2199, Val IoU: 0.3255\n",
      "  LR: 2.32e-05\n",
      "  âœ“ Saved best model (val_iou: 0.3255)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:49<00:00, 26.71it/s, loss=0.1681, avg_loss=0.2272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35/50 (0.9 min):\n",
      "  Train Loss: 0.2269, Train IoU: 0.3092\n",
      "  Val Loss: 0.2190, Val IoU: 0.3273\n",
      "  LR: 2.06e-05\n",
      "  âœ“ Saved best model (val_iou: 0.3273)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:48<00:00, 27.46it/s, loss=0.2488, avg_loss=0.2244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/50 (0.9 min):\n",
      "  Train Loss: 0.2244, Train IoU: 0.3153\n",
      "  Val Loss: 0.2191, Val IoU: 0.3280\n",
      "  LR: 1.81e-05\n",
      "  âœ“ Saved best model (val_iou: 0.3280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:45<00:00, 29.15it/s, loss=0.3038, avg_loss=0.2230]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37/50 (0.8 min):\n",
      "  Train Loss: 0.2230, Train IoU: 0.3179\n",
      "  Val Loss: 0.2164, Val IoU: 0.3360\n",
      "  LR: 1.58e-05\n",
      "  âœ“ Saved best model (val_iou: 0.3360)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:43<00:00, 30.58it/s, loss=0.1873, avg_loss=0.2208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38/50 (0.8 min):\n",
      "  Train Loss: 0.2208, Train IoU: 0.3228\n",
      "  Val Loss: 0.2147, Val IoU: 0.3373\n",
      "  LR: 1.36e-05\n",
      "  âœ“ Saved best model (val_iou: 0.3373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:48<00:00, 27.56it/s, loss=0.1898, avg_loss=0.2206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/50 (0.9 min):\n",
      "  Train Loss: 0.2206, Train IoU: 0.3231\n",
      "  Val Loss: 0.2130, Val IoU: 0.3425\n",
      "  LR: 1.15e-05\n",
      "  âœ“ Saved best model (val_iou: 0.3425)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:52<00:00, 25.57it/s, loss=0.2411, avg_loss=0.2203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40/50 (1.0 min):\n",
      "  Train Loss: 0.2205, Train IoU: 0.3232\n",
      "  Val Loss: 0.2114, Val IoU: 0.3460\n",
      "  LR: 9.55e-06\n",
      "  âœ“ Saved best model (val_iou: 0.3460)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:45<00:00, 29.28it/s, loss=0.2129, avg_loss=0.2188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41/50 (0.8 min):\n",
      "  Train Loss: 0.2185, Train IoU: 0.3282\n",
      "  Val Loss: 0.2116, Val IoU: 0.3448\n",
      "  LR: 7.78e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:45<00:00, 29.46it/s, loss=0.2117, avg_loss=0.2181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42/50 (0.8 min):\n",
      "  Train Loss: 0.2181, Train IoU: 0.3294\n",
      "  Val Loss: 0.2106, Val IoU: 0.3475\n",
      "  LR: 6.18e-06\n",
      "  âœ“ Saved best model (val_iou: 0.3475)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [00:49<00:00, 26.94it/s, loss=0.1839, avg_loss=0.2175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43/50 (1.0 min):\n",
      "  Train Loss: 0.2175, Train IoU: 0.3304\n",
      "  Val Loss: 0.2089, Val IoU: 0.3503\n",
      "  LR: 4.76e-06\n",
      "  âœ“ Saved best model (val_iou: 0.3503)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [01:08<00:00, 19.44it/s, loss=0.1893, avg_loss=0.2166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44/50 (1.3 min):\n",
      "  Train Loss: 0.2167, Train IoU: 0.3322\n",
      "  Val Loss: 0.2084, Val IoU: 0.3531\n",
      "  LR: 3.51e-06\n",
      "  âœ“ Saved best model (val_iou: 0.3531)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [01:13<00:00, 18.11it/s, loss=0.2275, avg_loss=0.2157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45/50 (1.4 min):\n",
      "  Train Loss: 0.2160, Train IoU: 0.3336\n",
      "  Val Loss: 0.2081, Val IoU: 0.3543\n",
      "  LR: 2.45e-06\n",
      "  âœ“ Saved best model (val_iou: 0.3543)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [01:11<00:00, 18.66it/s, loss=0.2224, avg_loss=0.2154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46/50 (1.3 min):\n",
      "  Train Loss: 0.2158, Train IoU: 0.3341\n",
      "  Val Loss: 0.2102, Val IoU: 0.3490\n",
      "  LR: 1.57e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [01:14<00:00, 17.91it/s, loss=0.2199, avg_loss=0.2144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47/50 (1.4 min):\n",
      "  Train Loss: 0.2145, Train IoU: 0.3371\n",
      "  Val Loss: 0.2089, Val IoU: 0.3519\n",
      "  LR: 8.86e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [01:10<00:00, 18.96it/s, loss=0.1781, avg_loss=0.2155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48/50 (1.3 min):\n",
      "  Train Loss: 0.2153, Train IoU: 0.3353\n",
      "  Val Loss: 0.2081, Val IoU: 0.3538\n",
      "  LR: 3.94e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [01:12<00:00, 18.39it/s, loss=0.1657, avg_loss=0.2145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49/50 (1.3 min):\n",
      "  Train Loss: 0.2146, Train IoU: 0.3369\n",
      "  Val Loss: 0.2086, Val IoU: 0.3521\n",
      "  LR: 9.87e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1334/1334 [01:15<00:00, 17.72it/s, loss=0.1760, avg_loss=0.2152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50/50 (1.4 min):\n",
      "  Train Loss: 0.2153, Train IoU: 0.3356\n",
      "  Val Loss: 0.2084, Val IoU: 0.3525\n",
      "  LR: 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# ======================\n",
    "# 1. SegFormer Model (Fixed)\n",
    "# ======================\n",
    "\n",
    "class SegFormerLandslide(nn.Module):\n",
    "    def __init__(self, num_classes=1, input_size=512, output_size=512):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Load pre-trained SegFormer\n",
    "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Add upsampling layer to match output size\n",
    "        self.upsample = nn.Upsample(size=output_size, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        print(\"âœ“ SegFormer initialized with pre-trained weights + upsampling\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = self.segformer(x).logits  # [B, 1, 128, 128]\n",
    "        return self.upsample(outputs)  # [B, 1, 512, 512]\n",
    "\n",
    "# ======================\n",
    "# 2. Loss Function\n",
    "# ======================\n",
    "\n",
    "class LandslideLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def dice_loss(self, pred, target):\n",
    "        smooth = 1.0\n",
    "        pred = torch.sigmoid(pred)\n",
    "        intersection = (pred * target).sum()\n",
    "        union = pred.sum() + target.sum()\n",
    "        return 1.0 - (2.0 * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        bce_loss = self.bce(pred, target)\n",
    "        dice_loss = self.dice_loss(pred, target)\n",
    "        return self.alpha * bce_loss + (1 - self.alpha) * dice_loss\n",
    "\n",
    "def calculate_iou(pred, target):\n",
    "    pred = (torch.sigmoid(pred) > 0.5).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    return (intersection + 1e-6) / (union + 1e-6)\n",
    "\n",
    "# ======================\n",
    "# 3. OPTIMIZED Dataset Class for RTX 4070\n",
    "# ======================\n",
    "\n",
    "class OptimizedLandslideH5Dataset(Dataset):\n",
    "    def __init__(self, img_h5_paths, mask_h5_paths=None, img_size=512, is_train=True, max_samples=None):\n",
    "        self.img_h5_paths = img_h5_paths\n",
    "        self.mask_h5_paths = mask_h5_paths\n",
    "        self.img_size = img_size\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Optimized transform - minimal operations\n",
    "        if is_train:\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(img_size, img_size),\n",
    "                A.HorizontalFlip(p=0.3),\n",
    "                A.VerticalFlip(p=0.3),\n",
    "                ToTensorV2(),\n",
    "            ], is_check_shapes=False)\n",
    "        else:\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(img_size, img_size),\n",
    "                ToTensorV2(),\n",
    "            ], is_check_shapes=False)\n",
    "        \n",
    "        # PRELOAD all file indices for maximum speed\n",
    "        self.total_samples = 0\n",
    "        self.file_indices = []\n",
    "        \n",
    "        print(\"Preloading file indices for fast access...\")\n",
    "        for file_idx, img_path in enumerate(tqdm(img_h5_paths, desc=\"Scanning HDF5 files\")):\n",
    "            try:\n",
    "                with h5py.File(img_path, 'r') as img_file:\n",
    "                    img_key = list(img_file.keys())[0]\n",
    "                    n_samples = img_file[img_key].shape[0]\n",
    "                    \n",
    "                    for sample_idx in range(n_samples):\n",
    "                        if max_samples and self.total_samples >= max_samples:\n",
    "                            break\n",
    "                        self.file_indices.append((file_idx, sample_idx))\n",
    "                        self.total_samples += 1\n",
    "                    \n",
    "                    if max_samples and self.total_samples >= max_samples:\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        print(f\"âœ“ Loaded {self.total_samples} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, sample_idx = self.file_indices[idx]\n",
    "        img_path = self.img_h5_paths[file_idx]\n",
    "        mask_path = self.mask_h5_paths[file_idx] if self.mask_h5_paths else None\n",
    "        \n",
    "        try:\n",
    "            # ULTRA-FAST loading - minimal error checking\n",
    "            with h5py.File(img_path, 'r') as img_file:\n",
    "                img_key = list(img_file.keys())[0]\n",
    "                img = img_file[img_key][sample_idx]\n",
    "                \n",
    "                # Quick format conversion\n",
    "                if len(img.shape) == 3 and img.shape[0] == 3:\n",
    "                    img = np.transpose(img, (1, 2, 0))\n",
    "                elif len(img.shape) == 2:\n",
    "                    img = np.stack([img, img, img], axis=-1)\n",
    "                \n",
    "                # Take first 3 channels and normalize\n",
    "                if img.shape[-1] > 3:\n",
    "                    img = img[:, :, :3]\n",
    "                img = img.astype(np.float32) / 255.0\n",
    "                \n",
    "                # Load mask\n",
    "                mask = np.zeros(img.shape[:2], dtype=np.float32)\n",
    "                if mask_path and os.path.exists(mask_path):\n",
    "                    with h5py.File(mask_path, 'r') as mask_file:\n",
    "                        mask_key = list(mask_file.keys())[0]\n",
    "                        mask_data = mask_file[mask_key]\n",
    "                        \n",
    "                        if len(mask_data.shape) == 3 and sample_idx < mask_data.shape[0]:\n",
    "                            mask = mask_data[sample_idx]\n",
    "                        elif len(mask_data.shape) == 2:\n",
    "                            mask = mask_data[:]\n",
    "                        \n",
    "                        if len(mask.shape) == 3:\n",
    "                            mask = mask.squeeze()\n",
    "                        mask = (mask > 0).astype(np.float32)\n",
    "                \n",
    "                # Apply transform\n",
    "                transformed = self.transform(image=img, mask=mask)\n",
    "                return {\n",
    "                    'image': transformed['image'],\n",
    "                    'mask': transformed['mask'].unsqueeze(0)\n",
    "                }\n",
    "                \n",
    "        except Exception:\n",
    "            # Fast fallback - zero tensor\n",
    "            dummy_img = torch.zeros(3, self.img_size, self.img_size)\n",
    "            dummy_mask = torch.zeros(1, self.img_size, self.img_size)\n",
    "            return {'image': dummy_img, 'mask': dummy_mask}\n",
    "\n",
    "# ======================\n",
    "# 4. SPEED TEST Function\n",
    "# ======================\n",
    "\n",
    "def speed_test():\n",
    "    \"\"\"Comprehensive speed test for RTX 4070\"\"\"\n",
    "    base_path = \"/home/neel/Geog_project/Landslide4Sense_1\"\n",
    "    \n",
    "    def find_h5_files(directory):\n",
    "        if not os.path.exists(directory):\n",
    "            return []\n",
    "        return sorted([os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.h5')])\n",
    "    \n",
    "    train_img_files = find_h5_files(os.path.join(base_path, \"TrainData\", \"img\"))\n",
    "    train_mask_files = find_h5_files(os.path.join(base_path, \"TrainData\", \"mask\"))\n",
    "    \n",
    "    print(f\"Found {len(train_img_files)} training files\")\n",
    "    print(f\"RTX 4070 detected - optimizing for high performance...\")\n",
    "    \n",
    "    # Test configurations optimized for RTX 4070\n",
    "    configs = [\n",
    "        {\"img_size\": 512, \"batch_size\": 8, \"max_samples\": 2000, \"num_workers\": 6},\n",
    "        {\"img_size\": 384, \"batch_size\": 12, \"max_samples\": 2000, \"num_workers\": 6},\n",
    "        {\"img_size\": 256, \"batch_size\": 16, \"max_samples\": 2000, \"num_workers\": 6},\n",
    "        {\"img_size\": 128, \"batch_size\": 32, \"max_samples\": 2000, \"num_workers\": 6},\n",
    "    ]\n",
    "    \n",
    "    best_config = None\n",
    "    best_speed = float('inf')\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing config: {config}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = OptimizedLandslideH5Dataset(\n",
    "            img_h5_paths=train_img_files[:20],  # More files for better test\n",
    "            mask_h5_paths=train_mask_files[:20],\n",
    "            img_size=config[\"img_size\"],\n",
    "            max_samples=config[\"max_samples\"]\n",
    "        )\n",
    "        \n",
    "        # Create optimized dataloader\n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=config[\"batch_size\"], \n",
    "            shuffle=True, \n",
    "            num_workers=config[\"num_workers\"],\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True  # Better for multiple epochs\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        model = SegFormerLandslide(num_classes=1, input_size=config[\"img_size\"], output_size=config[\"img_size\"])\n",
    "        device = torch.device('cuda')\n",
    "        model.to(device)\n",
    "        \n",
    "        # Warmup GPU\n",
    "        print(\"Warming up GPU...\")\n",
    "        dummy_input = torch.randn(config[\"batch_size\"], 3, config[\"img_size\"], config[\"img_size\"]).to(device)\n",
    "        for _ in range(10):\n",
    "            _ = model(dummy_input)\n",
    "        \n",
    "        # Time data loading\n",
    "        print(\"Testing data loading speed...\")\n",
    "        data_times = []\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= 20:  # Test 20 batches\n",
    "                break\n",
    "            start_time = time.time()\n",
    "            images = batch['image'].to(device, non_blocking=True)\n",
    "            masks = batch['mask'].to(device, non_blocking=True)\n",
    "            data_times.append(time.time() - start_time)\n",
    "        \n",
    "        avg_data_time = np.mean(data_times)\n",
    "        \n",
    "        # Time forward/backward pass\n",
    "        print(\"Testing training speed...\")\n",
    "        model.train()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "        criterion = LandslideLoss()\n",
    "        \n",
    "        training_times = []\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= 20:\n",
    "                break\n",
    "            images = batch['image'].to(device, non_blocking=True)\n",
    "            masks = batch['mask'].to(device, non_blocking=True)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_times.append(time.time() - start_time)\n",
    "        \n",
    "        avg_training_time = np.mean(training_times)\n",
    "        total_batch_time = avg_data_time + avg_training_time\n",
    "        \n",
    "        # Calculate estimated epoch time\n",
    "        samples_per_epoch = min(50000, len(dataset))  # Reasonable subset\n",
    "        batches_per_epoch = samples_per_epoch // config[\"batch_size\"]\n",
    "        estimated_epoch_time = total_batch_time * batches_per_epoch / 60  # in minutes\n",
    "        \n",
    "        print(f\"Results for {config['img_size']}x{config['img_size']}:\")\n",
    "        print(f\"  Data loading: {avg_data_time*1000:.1f}ms per batch\")\n",
    "        print(f\"  Training: {avg_training_time*1000:.1f}ms per batch\")\n",
    "        print(f\"  Total: {total_batch_time*1000:.1f}ms per batch\")\n",
    "        print(f\"  Estimated epoch: {estimated_epoch_time:.1f} minutes\")\n",
    "        print(f\"  GPU Memory: {torch.cuda.memory_allocated()/1024**3:.1f}GB\")\n",
    "        \n",
    "        if estimated_epoch_time < best_speed:\n",
    "            best_speed = estimated_epoch_time\n",
    "            best_config = config\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸŽ¯ BEST CONFIGURATION:\")\n",
    "    print(f\"   Image Size: {best_config['img_size']}x{best_config['img_size']}\")\n",
    "    print(f\"   Batch Size: {best_config['batch_size']}\")\n",
    "    print(f\"   Estimated Epoch Time: {best_speed:.1f} minutes\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "# ======================\n",
    "# 5. OPTIMIZED Training for RTX 4070\n",
    "# ======================\n",
    "\n",
    "def optimized_train_model(model, train_loader, val_loader, epochs=50, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Mixed precision for RTX 4070\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    criterion = LandslideLoss(alpha=0.7)\n",
    "    \n",
    "    best_val_iou = 0.0\n",
    "    \n",
    "    print(\"Starting optimized training with mixed precision...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training with mixed precision\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        \n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_bar):\n",
    "            images = batch['image'].to(device, non_blocking=True)\n",
    "            masks = batch['mask'].to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                if outputs.shape[-2:] != masks.shape[-2:]:\n",
    "                    outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=True)\n",
    "                loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Mixed precision backward\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_iou += calculate_iou(outputs, masks).item()\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                train_bar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'avg_loss': f'{train_loss/(batch_idx+1):.4f}'\n",
    "                })\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_iou = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(device, non_blocking=True)\n",
    "                masks = batch['mask'].to(device, non_blocking=True)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(images)\n",
    "                    if outputs.shape[-2:] != masks.shape[-2:]:\n",
    "                        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=True)\n",
    "                    loss = criterion(outputs, masks)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_iou += calculate_iou(outputs, masks).item()\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_iou = train_iou / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_iou = val_iou / len(val_loader)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}/{epochs} ({epoch_time/60:.1f} min):')\n",
    "        print(f'  Train Loss: {avg_train_loss:.4f}, Train IoU: {avg_train_iou:.4f}')\n",
    "        print(f'  Val Loss: {avg_val_loss:.4f}, Val IoU: {avg_val_iou:.4f}')\n",
    "        print(f'  LR: {scheduler.get_last_lr()[0]:.2e}')\n",
    "        \n",
    "        if avg_val_iou > best_val_iou:\n",
    "            best_val_iou = avg_val_iou\n",
    "            torch.save(model.state_dict(), 'best_landslide_model.pth')\n",
    "            print(f'  âœ“ Saved best model (val_iou: {avg_val_iou:.4f})')\n",
    "\n",
    "# ======================\n",
    "# 6. FAST TRAINING with Optimal Settings\n",
    "# ======================\n",
    "# ======================\n",
    "# ULTRA-FAST TRAINING with Optimal Settings\n",
    "# ======================\n",
    "\n",
    "def ultra_fast_training():\n",
    "    \"\"\"Ultra-fast training with optimal settings from speed test\"\"\"\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"ðŸš€ Starting ULTRA-FAST training on RTX 4070...\")\n",
    "    \n",
    "    base_path = \"/home/neel/Geog_project/Landslide4Sense_1\"\n",
    "    \n",
    "    def find_h5_files(directory):\n",
    "        if not os.path.exists(directory):\n",
    "            return []\n",
    "        return sorted([os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.h5')])\n",
    "    \n",
    "    train_img_files = find_h5_files(os.path.join(base_path, \"TrainData\", \"img\"))\n",
    "    train_mask_files = find_h5_files(os.path.join(base_path, \"TrainData\", \"mask\"))\n",
    "   \n",
    "    IMG_SIZE = 128  # Better detail than 128\n",
    "    BATCH_SIZE = 24  # Fits in 8GB VRAM with larger model\n",
    "    SUBSET_SIZE = 40000  # Good amount of data\n",
    "    NUM_WORKERS = min(os.cpu_count(), 24) # Use available cores, up to a max of 6\n",
    "    \n",
    "    print(\"Creating optimized datasets...\")\n",
    "    train_dataset = OptimizedLandslideH5Dataset(\n",
    "        img_h5_paths=train_img_files,\n",
    "        mask_h5_paths=train_mask_files,\n",
    "        img_size=IMG_SIZE,\n",
    "        max_samples=SUBSET_SIZE,\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    # Split for validation (80/20)\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Ultra-fast dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_subset, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_subset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Training on {len(train_subset)} samples\")\n",
    "    print(f\"Validating on {len(val_subset)} samples\")\n",
    "    print(f\"Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "    \n",
    "    # Calculate expected epoch time\n",
    "    expected_batch_time = 0.046  # From speed test (46.4ms)\n",
    "    expected_epoch_time = expected_batch_time * len(train_loader)\n",
    "    print(f\"Expected epoch time: {expected_epoch_time:.1f} seconds ({expected_epoch_time/60:.1f} minutes)\")\n",
    "    \n",
    "    # Model\n",
    "    model = SegFormerLandslide(num_classes=1, input_size=IMG_SIZE, output_size=IMG_SIZE)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model parameters: {total_params:,}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Training Summary:\")\n",
    "    print(f\"   â€¢ Expected epoch time: {expected_epoch_time:.1f} seconds\")\n",
    "    print(f\"   â€¢ Full training (50 epochs): {expected_epoch_time * 50 / 60:.1f} minutes\")\n",
    "    print(f\"   â€¢ GPU Memory usage: ~0.6GB\")\n",
    "    print(f\"   â€¢ Quality: Good for rapid prototyping\")\n",
    "    \n",
    "    print(\"\\nStarting ULTRA-FAST training...\")\n",
    "    optimized_train_model(model, train_loader, val_loader, epochs=50, device=device)\n",
    "\n",
    "# Run ultra-fast training\n",
    "ultra_fast_training()\n",
    "# ======================\n",
    "# MAIN EXECUTION\n",
    "# ======================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1b61fe-1e17-4480-b894-910aa451219a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SegFormer initialized with pre-trained weights + upsampling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/neel/Geog_project/my_model.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "# Example: Train a model\n",
    "\n",
    "model = SegFormerLandslide()\n",
    "\n",
    "\n",
    "# Define the filename\n",
    "filename = '/home/neel/Geog_project/my_model.joblib'\n",
    "\n",
    "# Export/Save the model to disk\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b269ace-6885-4c37-ba89-90d307f89d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SINGLE IMAGE TEST ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_trained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 491\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    488\u001b[39m \n\u001b[32m    489\u001b[39m     \u001b[38;5;66;03m# Example 1: Test single image (no ground truth needed)\u001b[39;00m\n\u001b[32m    490\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== SINGLE IMAGE TEST ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m     result = \u001b[43mtest_single_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/home/neel/Geog_project/best_landslide_model.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/home/neel/Geog_project/myanmar_ali_2015296.jpg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Optional: provide if you have ground truth\u001b[39;49;00m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m     \u001b[38;5;66;03m# Example 2: Comprehensive evaluation on dataset\u001b[39;00m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== COMPREHENSIVE DATASET EVALUATION ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 460\u001b[39m, in \u001b[36mtest_single_image\u001b[39m\u001b[34m(model_path, image_path, mask_path, threshold)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Quick test on a single image\"\"\"\u001b[39;00m\n\u001b[32m    459\u001b[39m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m model = \u001b[43mload_trained_model\u001b[49m(model_path)\n\u001b[32m    461\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    462\u001b[39m model.to(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_trained_model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, confusion_matrix\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# ======================\n",
    "# 1. COMPREHENSIVE EVALUATION METRICS\n",
    "# ======================\n",
    "\n",
    "class LandslideEvaluator:\n",
    "    def __init__(self, model, device='cuda'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "    \n",
    "    def calculate_metrics(self, pred, target, threshold=0.5):\n",
    "        \"\"\"Calculate comprehensive segmentation metrics\"\"\"\n",
    "        pred_binary = (pred > threshold).astype(np.float32)\n",
    "        target_binary = (target > 0.5).astype(np.float32)\n",
    "        \n",
    "        # Basic metrics\n",
    "        intersection = np.sum(pred_binary * target_binary)\n",
    "        union = np.sum(pred_binary) + np.sum(target_binary) - intersection\n",
    "        total_pixels = pred_binary.size\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        epsilon = 1e-6\n",
    "        \n",
    "        # Core metrics\n",
    "        iou = intersection / (union + epsilon)\n",
    "        dice = (2 * intersection) / (np.sum(pred_binary) + np.sum(target_binary) + epsilon)\n",
    "        \n",
    "        # Precision, Recall, F1\n",
    "        true_positives = intersection\n",
    "        false_positives = np.sum(pred_binary * (1 - target_binary))\n",
    "        false_negatives = np.sum((1 - pred_binary) * target_binary)\n",
    "        true_negatives = np.sum((1 - pred_binary) * (1 - target_binary))\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives + epsilon)\n",
    "        recall = true_positives / (true_positives + false_negatives + epsilon)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "        \n",
    "        # Accuracy\n",
    "        accuracy = (true_positives + true_negatives) / total_pixels\n",
    "        \n",
    "        # Specificity\n",
    "        specificity = true_negatives / (true_negatives + false_positives + epsilon)\n",
    "        \n",
    "        # Balanced Accuracy\n",
    "        balanced_accuracy = (recall + specificity) / 2\n",
    "        \n",
    "        return {\n",
    "            'iou': iou,\n",
    "            'dice': dice,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'accuracy': accuracy,\n",
    "            'specificity': specificity,\n",
    "            'balanced_accuracy': balanced_accuracy,\n",
    "            'true_positives': true_positives,\n",
    "            'false_positives': false_positives,\n",
    "            'false_negatives': false_negatives,\n",
    "            'true_negatives': true_negatives\n",
    "        }\n",
    "    \n",
    "    def evaluate_single_image(self, image_path, mask_path=None, threshold=0.5):\n",
    "        \"\"\"Evaluate model on a single image\"\"\"\n",
    "        # Load and preprocess image\n",
    "        image = self.load_image(image_path)\n",
    "        image_tensor = self.preprocess_image(image)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            prediction = self.model(image_tensor.unsqueeze(0).to(self.device))\n",
    "            prediction_prob = torch.sigmoid(prediction).squeeze().cpu().numpy()\n",
    "        \n",
    "        # Load ground truth if available\n",
    "        ground_truth = None\n",
    "        if mask_path and os.path.exists(mask_path):\n",
    "            ground_truth = self.load_mask(mask_path)\n",
    "        \n",
    "        # Calculate metrics if ground truth exists\n",
    "        metrics = None\n",
    "        if ground_truth is not None:\n",
    "            # Ensure same size\n",
    "            if prediction_prob.shape != ground_truth.shape:\n",
    "                ground_truth = cv2.resize(ground_truth, \n",
    "                                        (prediction_prob.shape[1], prediction_prob.shape[0]))\n",
    "            \n",
    "            metrics = self.calculate_metrics(prediction_prob, ground_truth, threshold)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'prediction': prediction_prob,\n",
    "            'ground_truth': ground_truth,\n",
    "            'metrics': metrics,\n",
    "            'binary_prediction': (prediction_prob > threshold).astype(np.float32)\n",
    "        }\n",
    "    \n",
    "    def evaluate_dataset(self, image_dir, mask_dir=None, threshold=0.5):\n",
    "        \"\"\"Evaluate model on entire dataset\"\"\"\n",
    "        image_files = [f for f in os.listdir(image_dir) if f.endswith(('.tif', '.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        all_metrics = []\n",
    "        results = []\n",
    "        \n",
    "        print(f\"Evaluating on {len(image_files)} images...\")\n",
    "        \n",
    "        for image_file in tqdm(image_files):\n",
    "            image_path = os.path.join(image_dir, image_file)\n",
    "            \n",
    "            # Find corresponding mask\n",
    "            mask_path = None\n",
    "            if mask_dir:\n",
    "                mask_candidates = [\n",
    "                    image_file.replace('.tif', '.png').replace('.jpg', '.png').replace('.jpeg', '.png'),\n",
    "                    image_file,\n",
    "                    image_file.replace('.png', '.tif')\n",
    "                ]\n",
    "                \n",
    "                for mask_candidate in mask_candidates:\n",
    "                    candidate_path = os.path.join(mask_dir, mask_candidate)\n",
    "                    if os.path.exists(candidate_path):\n",
    "                        mask_path = candidate_path\n",
    "                        break\n",
    "            \n",
    "            try:\n",
    "                result = self.evaluate_single_image(image_path, mask_path, threshold)\n",
    "                \n",
    "                if result['metrics'] is not None:\n",
    "                    all_metrics.append(result['metrics'])\n",
    "                \n",
    "                results.append({\n",
    "                    'image_file': image_file,\n",
    "                    'metrics': result['metrics'],\n",
    "                    'has_ground_truth': mask_path is not None\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return results, all_metrics\n",
    "    \n",
    "    def load_image(self, image_path):\n",
    "        \"\"\"Load image in various formats\"\"\"\n",
    "        if image_path.endswith('.tif'):\n",
    "            with rasterio.open(image_path) as src:\n",
    "                image = src.read()[:3]  # Take first 3 bands\n",
    "                image = np.transpose(image, (1, 2, 0))\n",
    "        else:\n",
    "            image = np.array(Image.open(image_path).convert('RGB'))\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def load_mask(self, mask_path):\n",
    "        \"\"\"Load mask in various formats\"\"\"\n",
    "        if mask_path.endswith('.tif'):\n",
    "            with rasterio.open(mask_path) as src:\n",
    "                mask = src.read(1)\n",
    "        else:\n",
    "            mask = np.array(Image.open(mask_path).convert('L'))\n",
    "        \n",
    "        # Binarize mask\n",
    "        mask = (mask > 0).astype(np.float32)\n",
    "        return mask\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"Preprocess image for model input\"\"\"\n",
    "        # Resize to model input size\n",
    "        if hasattr(self.model, 'input_size'):\n",
    "            target_size = self.model.input_size\n",
    "        else:\n",
    "            target_size = 256  # Default\n",
    "        \n",
    "        image_resized = cv2.resize(image, (target_size, target_size))\n",
    "        \n",
    "        # Normalize\n",
    "        image_normalized = image_resized.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image_tensor = torch.from_numpy(image_normalized).permute(2, 0, 1).float()\n",
    "        \n",
    "        return image_tensor\n",
    "\n",
    "# ======================\n",
    "# 2. VISUALIZATION TOOLS\n",
    "# ======================\n",
    "\n",
    "class ResultVisualizer:\n",
    "    def __init__(self):\n",
    "        self.cmap = plt.cm.viridis\n",
    "    \n",
    "    def plot_single_result(self, result, threshold=0.5, save_path=None):\n",
    "        \"\"\"Plot single image result with prediction\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        image = result['image']\n",
    "        prediction = result['prediction']\n",
    "        ground_truth = result['ground_truth']\n",
    "        binary_pred = result['binary_prediction']\n",
    "        metrics = result['metrics']\n",
    "        \n",
    "        # Original image\n",
    "        axes[0].imshow(image)\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Prediction probability\n",
    "        im1 = axes[1].imshow(prediction, cmap='viridis', vmin=0, vmax=1)\n",
    "        axes[1].set_title('Prediction Probability')\n",
    "        axes[1].axis('off')\n",
    "        plt.colorbar(im1, ax=axes[1])\n",
    "        \n",
    "        # Binary prediction\n",
    "        axes[2].imshow(binary_pred, cmap='gray')\n",
    "        axes[2].set_title(f'Binary Prediction (threshold={threshold})')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        # Ground truth (if available)\n",
    "        if ground_truth is not None:\n",
    "            axes[3].imshow(ground_truth, cmap='gray')\n",
    "            axes[3].set_title('Ground Truth')\n",
    "            axes[3].axis('off')\n",
    "            \n",
    "            # Overlay prediction on image\n",
    "            axes[4].imshow(image)\n",
    "            axes[4].imshow(binary_pred, alpha=0.5, cmap='Reds')\n",
    "            axes[4].set_title('Prediction Overlay (Red)')\n",
    "            axes[4].axis('off')\n",
    "            \n",
    "            # Difference map\n",
    "            difference = np.abs(binary_pred - ground_truth)\n",
    "            axes[5].imshow(difference, cmap='coolwarm')\n",
    "            axes[5].set_title('Difference Map\\n(Red=False Pos, Blue=False Neg)')\n",
    "            axes[5].axis('off')\n",
    "            \n",
    "            # Add metrics text\n",
    "            if metrics:\n",
    "                metrics_text = f\"IoU: {metrics['iou']:.3f}\\nDice: {metrics['dice']:.3f}\\nF1: {metrics['f1_score']:.3f}\\nPrecision: {metrics['precision']:.3f}\\nRecall: {metrics['recall']:.3f}\"\n",
    "                fig.text(0.02, 0.5, metrics_text, fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "        \n",
    "        else:\n",
    "            # If no ground truth, show different visualizations\n",
    "            axes[3].axis('off')\n",
    "            \n",
    "            # Overlay probability on image\n",
    "            axes[4].imshow(image)\n",
    "            axes[4].imshow(prediction, alpha=0.6, cmap='viridis')\n",
    "            axes[4].set_title('Probability Overlay')\n",
    "            axes[4].axis('off')\n",
    "            \n",
    "            # Confidence histogram\n",
    "            axes[5].hist(prediction.flatten(), bins=50, alpha=0.7)\n",
    "            axes[5].set_title('Prediction Confidence Distribution')\n",
    "            axes[5].set_xlabel('Confidence')\n",
    "            axes[5].set_ylabel('Frequency')\n",
    "            axes[5].axvline(threshold, color='red', linestyle='--', label=f'Threshold ({threshold})')\n",
    "            axes[5].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved visualization to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def plot_metrics_distribution(self, all_metrics, save_path=None):\n",
    "        \"\"\"Plot distribution of metrics across dataset\"\"\"\n",
    "        if not all_metrics:\n",
    "            print(\"No metrics to plot\")\n",
    "            return\n",
    "        \n",
    "        metrics_df = pd.DataFrame(all_metrics)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        # Plot distributions\n",
    "        metrics_to_plot = ['iou', 'dice', 'f1_score', 'precision', 'recall', 'accuracy']\n",
    "        titles = ['IoU Score', 'Dice Coefficient', 'F1 Score', 'Precision', 'Recall', 'Accuracy']\n",
    "        \n",
    "        for i, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "            axes[i].hist(metrics_df[metric], bins=20, alpha=0.7, edgecolor='black')\n",
    "            axes[i].set_title(f'{title} Distribution')\n",
    "            axes[i].set_xlabel(title)\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            axes[i].axvline(metrics_df[metric].mean(), color='red', linestyle='--', \n",
    "                          label=f'Mean: {metrics_df[metric].mean():.3f}')\n",
    "            axes[i].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return metrics_df\n",
    "    \n",
    "    def plot_confusion_matrix(self, all_metrics, save_path=None):\n",
    "        \"\"\"Plot confusion matrix from aggregated metrics\"\"\"\n",
    "        if not all_metrics:\n",
    "            print(\"No metrics for confusion matrix\")\n",
    "            return\n",
    "        \n",
    "        total_tp = sum(m['true_positives'] for m in all_metrics)\n",
    "        total_fp = sum(m['false_positives'] for m in all_metrics)\n",
    "        total_fn = sum(m['false_negatives'] for m in all_metrics)\n",
    "        total_tn = sum(m['true_negatives'] for m in all_metrics)\n",
    "        \n",
    "        cm = np.array([[total_tn, total_fp], \n",
    "                      [total_fn, total_tp]])\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='.0f', cmap='Blues',\n",
    "                   xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
    "                   yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "        plt.title('Confusion Matrix (Aggregated)')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# ======================\n",
    "# 3. THRESHOLD OPTIMIZATION\n",
    "# ======================\n",
    "\n",
    "def find_optimal_threshold(all_metrics, all_predictions, all_ground_truths):\n",
    "    \"\"\"Find optimal threshold using precision-recall curve\"\"\"\n",
    "    # Flatten all predictions and ground truths\n",
    "    all_pred_flat = np.concatenate([pred.flatten() for pred in all_predictions])\n",
    "    all_gt_flat = np.concatenate([gt.flatten() for gt in all_ground_truths])\n",
    "    \n",
    "    # Calculate precision-recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(all_gt_flat, all_pred_flat)\n",
    "    \n",
    "    # Calculate F1 score for each threshold\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    \n",
    "    # Find optimal threshold (max F1)\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_f1 = f1_scores[optimal_idx]\n",
    "    \n",
    "    # Plot PR curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(recall, precision, linewidth=2, label='Precision-Recall Curve')\n",
    "    plt.plot(recall[optimal_idx], precision[optimal_idx], 'ro', \n",
    "             label=f'Optimal (F1={optimal_f1:.3f}, threshold={optimal_threshold:.3f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return optimal_threshold, optimal_f1\n",
    "\n",
    "# ======================\n",
    "# 4. COMPLETE EVALUATION PIPELINE\n",
    "# ======================\n",
    "\n",
    "def comprehensive_evaluation(model_path, test_image_dir, test_mask_dir=None, output_dir='evaluation_results'):\n",
    "    \"\"\"Complete evaluation pipeline\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load model\n",
    "    print(\"Loading trained model...\")\n",
    "    model = load_trained_model(model_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize evaluator and visualizer\n",
    "    evaluator = LandslideEvaluator(model, device)\n",
    "    visualizer = ResultVisualizer()\n",
    "    \n",
    "    # Evaluate dataset\n",
    "    print(\"Evaluating on test dataset...\")\n",
    "    results, all_metrics = evaluator.evaluate_dataset(test_image_dir, test_mask_dir)\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame([r for r in results if r['metrics'] is not None])\n",
    "    results_df.to_csv(os.path.join(output_dir, 'evaluation_results.csv'), index=False)\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    if all_metrics:\n",
    "        metrics_df = visualizer.plot_metrics_distribution(all_metrics, \n",
    "                                                         os.path.join(output_dir, 'metrics_distribution.png'))\n",
    "        \n",
    "        visualizer.plot_confusion_matrix(all_metrics, \n",
    "                                       os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "        \n",
    "        # Calculate overall statistics\n",
    "        overall_stats = {\n",
    "            'mean_iou': np.mean([m['iou'] for m in all_metrics]),\n",
    "            'mean_dice': np.mean([m['dice'] for m in all_metrics]),\n",
    "            'mean_f1': np.mean([m['f1_score'] for m in all_metrics]),\n",
    "            'mean_precision': np.mean([m['precision'] for m in all_metrics]),\n",
    "            'mean_recall': np.mean([m['recall'] for m in all_metrics]),\n",
    "            'mean_accuracy': np.mean([m['accuracy'] for m in all_metrics]),\n",
    "            'std_iou': np.std([m['iou'] for m in all_metrics]),\n",
    "            'num_samples': len(all_metrics)\n",
    "        }\n",
    "        \n",
    "        # Save overall statistics\n",
    "        stats_df = pd.DataFrame([overall_stats])\n",
    "        stats_df.to_csv(os.path.join(output_dir, 'overall_statistics.csv'), index=False)\n",
    "        \n",
    "        print(\"\\nðŸ“Š OVERALL PERFORMANCE SUMMARY:\")\n",
    "        print(f\"   â€¢ Number of samples: {overall_stats['num_samples']}\")\n",
    "        print(f\"   â€¢ Mean IoU: {overall_stats['mean_iou']:.4f} Â± {overall_stats['std_iou']:.4f}\")\n",
    "        print(f\"   â€¢ Mean Dice: {overall_stats['mean_dice']:.4f}\")\n",
    "        print(f\"   â€¢ Mean F1: {overall_stats['mean_f1']:.4f}\")\n",
    "        print(f\"   â€¢ Mean Precision: {overall_stats['mean_precision']:.4f}\")\n",
    "        print(f\"   â€¢ Mean Recall: {overall_stats['mean_recall']:.4f}\")\n",
    "        print(f\"   â€¢ Mean Accuracy: {overall_stats['mean_accuracy']:.4f}\")\n",
    "    \n",
    "    # Visualize some sample results\n",
    "    print(\"\\nGenerating sample visualizations...\")\n",
    "    sample_results = [r for r in results if r['has_ground_truth']][:5]  # First 5 with ground truth\n",
    "    \n",
    "    for i, result_info in enumerate(sample_results):\n",
    "        image_path = os.path.join(test_image_dir, result_info['image_file'])\n",
    "        mask_path = os.path.join(test_mask_dir, result_info['image_file']) if test_mask_dir else None\n",
    "        \n",
    "        result = evaluator.evaluate_single_image(image_path, mask_path)\n",
    "        visualizer.plot_single_result(\n",
    "            result, \n",
    "            save_path=os.path.join(output_dir, f'sample_result_{i+1}.png')\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nâœ… Evaluation complete! Results saved to: {output_dir}\")\n",
    "    return results, all_metrics\n",
    "\n",
    "# ======================\n",
    "# 5. QUICK SINGLE IMAGE TEST\n",
    "# ======================\n",
    "\n",
    "def test_single_image(model_path, image_path, mask_path=None, threshold=0.5):\n",
    "    \"\"\"Quick test on a single image\"\"\"\n",
    "    \n",
    "    # Load model\n",
    "    model = load_trained_model(model_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize evaluator and visualizer\n",
    "    evaluator = LandslideEvaluator(model, device)\n",
    "    visualizer = ResultVisualizer()\n",
    "    \n",
    "    # Evaluate single image\n",
    "    result = evaluator.evaluate_single_image(image_path, mask_path, threshold)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualizer.plot_single_result(result, threshold)\n",
    "    \n",
    "    # Print metrics if available\n",
    "    if result['metrics']:\n",
    "        print(\"\\nðŸ“ˆ PERFORMANCE METRICS:\")\n",
    "        for metric, value in result['metrics'].items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"   â€¢ {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ======================\n",
    "# 6. USAGE EXAMPLES\n",
    "# ======================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Example 1: Test single image (no ground truth needed)\n",
    "    print(\"=== SINGLE IMAGE TEST ===\")\n",
    "    result = test_single_image(\n",
    "        model_path='/home/neel/Geog_project/best_landslide_model.pth',\n",
    "        image_path='/home/neel/Geog_project/myanmar_ali_2015296.jpg',\n",
    "        mask_path=None,  # Optional: provide if you have ground truth\n",
    "        threshold=0.5\n",
    "    )\n",
    "    \n",
    "    # Example 2: Comprehensive evaluation on dataset\n",
    "    print(\"\\n=== COMPREHENSIVE DATASET EVALUATION ===\")\n",
    "    results, metrics = comprehensive_evaluation(\n",
    "        model_path='/home/neel/Geog_project/my_model.joblib',\n",
    "        test_image_dir='/path/to/your/test/images',\n",
    "        test_mask_dir='/path/to/your/test/masks',  # Optional\n",
    "        output_dir='evaluation_results'\n",
    "    )\n",
    "    \n",
    "    # Example 3: Batch process multiple images without ground truth\n",
    "    print(\"\\n=== BATCH PREDICTION (NO GROUND TRUTH) ===\")\n",
    "    model = load_trained_model('best_landslide_model_improved.pth')\n",
    "    evaluator = LandslideEvaluator(model)\n",
    "    \n",
    "    image_dir = 'Geog_project/myanmar_ali_2015296.jpg'\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.tif', '.png', '.jpg'))]\n",
    "    \n",
    "    for image_file in image_files[:3]:  # Test first 3 images\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        print(f\"\\nProcessing: {image_file}\")\n",
    "        \n",
    "        result = evaluator.evaluate_single_image(image_path)\n",
    "        \n",
    "        # Save prediction\n",
    "        pred_image = (result['prediction'] * 255).astype(np.uint8)\n",
    "        output_path = os.path.join('predictions', f'pred_{image_file}')\n",
    "        os.makedirs('predictions', exist_ok=True)\n",
    "        Image.fromarray(pred_image).save(output_path)\n",
    "        print(f\"Saved prediction to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0c3ed-c747-4cb8-adfb-7851dd0cbb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geo_env_new)",
   "language": "python",
   "name": "geo_env_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
